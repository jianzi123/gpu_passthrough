# GPU é›†ç¾¤éƒ¨ç½²å’Œç®¡ç†æœ€ä½³å®è·µ

æœ¬æ–‡æ¡£æä¾›åŸºäºæœ¬é¡¹ç›®çš„ GPU é›†ç¾¤éƒ¨ç½²ã€é…ç½®ã€ä¼˜åŒ–å’Œç»´æŠ¤çš„å®Œæ•´æœ€ä½³å®è·µæŒ‡å—ã€‚

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [åœºæ™¯åŒ–éƒ¨ç½²æŒ‡å—](#åœºæ™¯åŒ–éƒ¨ç½²æŒ‡å—)
3. [åˆå§‹éƒ¨ç½²æµç¨‹](#åˆå§‹éƒ¨ç½²æµç¨‹)
4. [é©±åŠ¨å®‰è£…æœ€ä½³å®è·µ](#é©±åŠ¨å®‰è£…æœ€ä½³å®è·µ)
5. [æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ](#æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ)
6. [éªŒè¯å’Œæµ‹è¯•æœ€ä½³å®è·µ](#éªŒè¯å’Œæµ‹è¯•æœ€ä½³å®è·µ)
7. [ç›‘æ§å’Œç»´æŠ¤](#ç›‘æ§å’Œç»´æŠ¤)
8. [å®‰å…¨æœ€ä½³å®è·µ](#å®‰å…¨æœ€ä½³å®è·µ)
9. [æ•…éšœæ’é™¤æµç¨‹](#æ•…éšœæ’é™¤æµç¨‹)
10. [ç‰ˆæœ¬ç®¡ç†å’Œå‡çº§](#ç‰ˆæœ¬ç®¡ç†å’Œå‡çº§)
11. [æ–‡æ¡£å’Œè®°å½•](#æ–‡æ¡£å’Œè®°å½•)
12. [å›¢é˜Ÿåä½œ](#å›¢é˜Ÿåä½œ)

---

## æ¦‚è¿°

### é¡¹ç›®åŠŸèƒ½æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU é›†ç¾¤ç®¡ç†å¹³å°                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. é©±åŠ¨ç®¡ç†     â”‚ Native/Container/Precompiled ä¸‰ç§æ–¹æ³•   â”‚
â”‚  2. è‡ªåŠ¨æ£€æµ‹     â”‚ GPU å‹å· â†’ CUDA ç‰ˆæœ¬è‡ªåŠ¨åŒ¹é…            â”‚
â”‚  3. CPU ä¼˜åŒ–     â”‚ NUMA/Governor/Turbo/C-States           â”‚
â”‚  4. ç³»ç»ŸéªŒè¯     â”‚ 8 å¤§ç±»å®Œæ•´éªŒè¯                          â”‚
â”‚  5. å¸¦å®½æµ‹è¯•     â”‚ PCIe/NVLink/RDMA æµ‹è¯•                  â”‚
â”‚  6. æ€§èƒ½åŸºå‡†     â”‚ NCCL/Megatron è®­ç»ƒåŸºå‡†                 â”‚
â”‚  7. NGC ç®¡ç†     â”‚ PyTorch/NeMo/Triton é•œåƒç®¡ç†           â”‚
â”‚  8. é¢„ç¼–è¯‘é©±åŠ¨   â”‚ å¿«é€Ÿéƒ¨ç½²ï¼Œ93% æ—¶é—´èŠ‚çœ                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒä»·å€¼ä¸»å¼ 

- âš¡ **å¿«é€Ÿéƒ¨ç½²**: é¢„ç¼–è¯‘é©±åŠ¨å®ç° 93% æ—¶é—´èŠ‚çœ
- ğŸ¯ **æ™ºèƒ½é…ç½®**: è‡ªåŠ¨æ£€æµ‹ GPU å¹¶é€‰æ‹©æœ€ä½³ CUDA ç‰ˆæœ¬
- ğŸš€ **æ€§èƒ½æå‡**: CPU ä¼˜åŒ–å¸¦æ¥ 20-40% æ€§èƒ½æå‡
- âœ… **å®Œæ•´éªŒè¯**: 8 å¤§ç±»éªŒè¯ç¡®ä¿ç³»ç»Ÿå°±ç»ª
- ğŸ“Š **åŸºå‡†æµ‹è¯•**: å†…ç½®æ€§èƒ½åŸºçº¿å¯¹æ¯”
- ğŸ”§ **æ˜“äºç®¡ç†**: ç»Ÿä¸€çš„å·¥å…·å’Œæ¥å£

---

## åœºæ™¯åŒ–éƒ¨ç½²æŒ‡å—

### åœºæ™¯ 1: å°å‹ç ”å‘å›¢é˜Ÿï¼ˆ1-10 å° GPU æœåŠ¡å™¨ï¼‰

**ç‰¹ç‚¹**:
- æœåŠ¡å™¨æ•°é‡å°‘
- ç®¡ç†ç›¸å¯¹ç®€å•
- éœ€è¦å¿«é€Ÿä¸Šæ‰‹

**æ¨èé…ç½®**:

```yaml
# éƒ¨ç½²ç­–ç•¥
driver_installation_method: native
auto_detect_cuda_version: true
enable_cpu_optimization: true
enable_monitoring: basic

# NGC é•œåƒ
ngc_images_to_pull:
  - pytorch:24.01
  - jupyter

# éªŒè¯çº§åˆ«
validation_level: quick
```

**éƒ¨ç½²å‘½ä»¤**:

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repo-url>
cd gpu_passthrough

# 2. é…ç½® Ansible inventory
cat > ansible/inventory/hosts << EOF
[gpu_nodes]
gpu-server-01 ansible_host=192.168.1.101
gpu-server-02 ansible_host=192.168.1.102
EOF

# 3. ä¸€é”®éƒ¨ç½²
cd ansible
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml

# 4. éªŒè¯
ansible -i inventory/hosts gpu_nodes -m shell -a "nvidia-smi"
```

**é¢„æœŸæ—¶é—´**: 30-45 åˆ†é’Ÿï¼ˆåŒ…å«é‡å¯ï¼‰

---

### åœºæ™¯ 2: ä¸­å‹ AI è®­ç»ƒé›†ç¾¤ï¼ˆ10-100 å°æœåŠ¡å™¨ï¼‰

**ç‰¹ç‚¹**:
- ä¸­ç­‰è§„æ¨¡
- éœ€è¦è‡ªåŠ¨åŒ–
- æ€§èƒ½è¦æ±‚é«˜
- éœ€è¦ç›‘æ§

**æ¨èé…ç½®**:

```yaml
# éƒ¨ç½²ç­–ç•¥
driver_installation_method: precompiled  # ä½¿ç”¨é¢„ç¼–è¯‘é©±åŠ¨
auto_detect_cuda_version: true
enable_cpu_optimization: true
enable_monitoring: full

# CPU ä¼˜åŒ–
cpu_governor: performance
enable_turbo_boost: true
optimize_numa: true

# NGC é•œåƒ
ngc_images_to_pull:
  - pytorch:24.01
  - nemo:24.01
  - tensorboard

# éªŒè¯å’Œæµ‹è¯•
validation_level: full
run_bandwidth_tests: true
run_nccl_tests: true

# ç›‘æ§
enable_dcgm: true
enable_prometheus_exporter: true
```

**éƒ¨ç½²æµç¨‹**:

```bash
# é˜¶æ®µ 1: é¢„æ„å»ºé¢„ç¼–è¯‘é©±åŠ¨ï¼ˆä¸€æ¬¡æ€§ï¼‰
./scripts/install/batch_build_drivers.sh

# é˜¶æ®µ 2: éƒ¨ç½²åˆ°æµ‹è¯•èŠ‚ç‚¹
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml \
  --limit test_nodes

# é˜¶æ®µ 3: éªŒè¯æµ‹è¯•èŠ‚ç‚¹
ansible-playbook -i inventory/hosts playbooks/validate_gpu.yml \
  --limit test_nodes

# é˜¶æ®µ 4: è¿è¡ŒåŸºå‡†æµ‹è¯•
ssh test-node-01 "sudo /usr/local/bin/gpu-benchmark bandwidth"
ssh test-node-01 "sudo /usr/local/bin/gpu-benchmark nccl"

# é˜¶æ®µ 5: æ‰¹é‡éƒ¨ç½²åˆ°ç”Ÿäº§ï¼ˆåˆ†æ‰¹ï¼‰
for batch in batch1 batch2 batch3; do
  ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml \
    --limit $batch \
    --forks 10
  sleep 300  # ç­‰å¾… 5 åˆ†é’Ÿè§‚å¯Ÿ
done

# é˜¶æ®µ 6: å…¨é›†ç¾¤éªŒè¯
ansible-playbook -i inventory/hosts playbooks/validate_gpu.yml
```

**é¢„æœŸæ—¶é—´**:
- é¢„æ„å»º: 1-2 å°æ—¶
- éƒ¨ç½²ï¼ˆå¹¶è¡Œ 10 èŠ‚ç‚¹ï¼‰: 20-30 åˆ†é’Ÿ/æ‰¹æ¬¡

---

### åœºæ™¯ 3: å¤§è§„æ¨¡ç”Ÿäº§é›†ç¾¤ï¼ˆ100+ å°æœåŠ¡å™¨ï¼‰

**ç‰¹ç‚¹**:
- å¤§è§„æ¨¡éƒ¨ç½²
- é«˜å¯ç”¨è¦æ±‚
- ä¸¥æ ¼çš„å˜æ›´ç®¡ç†
- éœ€è¦å®Œæ•´çš„ç›‘æ§å’Œå‘Šè­¦

**æ¨èé…ç½®**:

```yaml
# éƒ¨ç½²ç­–ç•¥
driver_installation_method: precompiled
auto_detect_cuda_version: true
enable_cpu_optimization: true
enable_monitoring: enterprise

# é«˜å¯ç”¨é…ç½®
driver_rollback_enabled: true
health_check_interval: 300
auto_recovery: true

# åˆ†é˜¶æ®µéƒ¨ç½²
deployment_strategy: canary  # é‡‘ä¸é›€éƒ¨ç½²
canary_percentage: 5
canary_validation_time: 3600  # 1 å°æ—¶

# ç›‘æ§å‘Šè­¦
enable_dcgm: true
enable_prometheus: true
enable_grafana: true
alert_on_errors: true
alert_channels: ["slack", "email", "pagerduty"]

# åˆè§„æ€§
audit_logging: true
change_tracking: true
```

**ä¼ä¸šçº§éƒ¨ç½²æµç¨‹**:

```bash
# 1. å‡†å¤‡é˜¶æ®µ
# 1.1 æ„å»ºé¢„ç¼–è¯‘é©±åŠ¨çŸ©é˜µ
./scripts/install/batch_build_drivers.sh

# 1.2 å»ºç«‹é©±åŠ¨ä»“åº“
rsync -av /opt/precompiled-drivers/ repo-server:/var/www/drivers/

# 1.3 éªŒè¯ä»“åº“å¯è®¿é—®æ€§
curl http://repo-server/drivers/index.json

# 2. é‡‘ä¸é›€éƒ¨ç½²ï¼ˆ5%ï¼‰
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml \
  --limit canary_nodes \
  --extra-vars "deployment_phase=canary"

# 3. é‡‘ä¸é›€éªŒè¯ï¼ˆ1å°æ—¶ï¼‰
for i in {1..12}; do
  ansible -i inventory/hosts canary_nodes -m shell \
    -a "/usr/local/bin/check-driver-health.sh"
  sleep 300
done

# 4. è“ç»¿éƒ¨ç½²åˆ°ç”Ÿäº§
# éƒ¨ç½²åˆ°è“ç»„ï¼ˆ50%ï¼‰
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml \
  --limit blue_nodes \
  --forks 20

# éªŒè¯è“ç»„
./scripts/validation/cluster_health_check.sh blue_nodes

# éƒ¨ç½²åˆ°ç»¿ç»„ï¼ˆå‰©ä½™ 50%ï¼‰
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml \
  --limit green_nodes \
  --forks 20

# 5. å…¨é›†ç¾¤éªŒè¯
ansible-playbook -i inventory/hosts playbooks/validate_gpu.yml \
  --extra-vars "validation_level=full"

# 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
ansible -i inventory/hosts all_gpu_nodes -m shell \
  -a "gpu-benchmark bandwidth" > benchmark_results.txt

# 7. ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š
./scripts/utils/generate_deployment_report.sh
```

**é¢„æœŸæ—¶é—´**:
- å‡†å¤‡: 2-4 å°æ—¶
- é‡‘ä¸é›€: 2 å°æ—¶ï¼ˆåŒ…å«éªŒè¯ï¼‰
- è“ç»¿éƒ¨ç½²: 3-5 å°æ—¶
- æ€»è®¡: ~8-12 å°æ—¶

---

### åœºæ™¯ 4: Kubernetes GPU é›†ç¾¤

**ç‰¹ç‚¹**:
- å®¹å™¨åŒ–ç¯å¢ƒ
- åŠ¨æ€è°ƒåº¦
- ä½¿ç”¨ GPU Operator

**æ¨èé…ç½®**:

```yaml
# é©±åŠ¨ç­–ç•¥
driver_installation_method: driver-container
driver_container_image: nvcr.io/nvidia/driver
driver_container_tag: 535.154.05-ubuntu22.04

# GPU Operator é›†æˆ
use_gpu_operator: true
gpu_operator_version: v24.3.0

# NGC é•œåƒ
ngc_images_to_pull:
  - pytorch:24.01
  - nemo:24.01
  - triton:24.01
```

**éƒ¨ç½²æµç¨‹**:

```bash
# 1. å‡†å¤‡ Kubernetes èŠ‚ç‚¹
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml \
  -e "driver_installation_method=driver-container"

# 2. å®‰è£… GPU Operator
helm repo add nvidia https://nvidia.github.io/gpu-operator
helm repo update

helm install gpu-operator nvidia/gpu-operator \
  -n gpu-operator-resources \
  --create-namespace \
  -f gpu-operator-values.yaml

# 3. éªŒè¯ GPU Operator
kubectl get pods -n gpu-operator-resources

# 4. æµ‹è¯• GPU è®¿é—®
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  containers:
  - name: cuda
    image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
EOF

kubectl logs gpu-test
```

---

## åˆå§‹éƒ¨ç½²æµç¨‹

### æ ‡å‡†éƒ¨ç½²æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ç¯å¢ƒå‡†å¤‡     â”‚
â”‚  - æ£€æŸ¥ç¡¬ä»¶      â”‚
â”‚  - ç½‘ç»œé…ç½®      â”‚
â”‚  - SSH è®¿é—®      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. æ£€æµ‹ GPU     â”‚
â”‚  - è‡ªåŠ¨æ£€æµ‹å‹å·  â”‚
â”‚  - é€‰æ‹© CUDA ç‰ˆæœ¬â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. å®‰è£…é©±åŠ¨     â”‚
â”‚  - Native/      â”‚
â”‚    Container/   â”‚
â”‚    Precompiled  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. CPU ä¼˜åŒ–     â”‚
â”‚  - NUMA é…ç½®     â”‚
â”‚  - Governor è®¾ç½® â”‚
â”‚  - Turbo å¯ç”¨    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. ç³»ç»ŸéªŒè¯     â”‚
â”‚  - 8 ç±»æ£€æŸ¥      â”‚
â”‚  - åŸºå‡†æµ‹è¯•      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. NGC é•œåƒ     â”‚
â”‚  - æ‹‰å–é•œåƒ      â”‚
â”‚  - æµ‹è¯•é•œåƒ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. äº¤ä»˜éªŒæ”¶     â”‚
â”‚  - ç”ŸæˆæŠ¥å‘Š      â”‚
â”‚  - ç§»äº¤æ–‡æ¡£      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è¯¦ç»†éƒ¨ç½²æ­¥éª¤

#### æ­¥éª¤ 1: ç¯å¢ƒå‡†å¤‡æ¸…å•

```bash
#!/bin/bash
# ç¯å¢ƒå‡†å¤‡æ£€æŸ¥è„šæœ¬

echo "=== ç¯å¢ƒå‡†å¤‡æ£€æŸ¥ ==="

# 1. æ£€æŸ¥ç¡¬ä»¶
check_hardware() {
    echo "1. ç¡¬ä»¶æ£€æŸ¥"

    # CPU
    echo "  CPU: $(lscpu | grep "Model name" | cut -d: -f2 | xargs)"
    echo "  CPU Cores: $(nproc)"

    # å†…å­˜
    echo "  Memory: $(free -h | grep Mem | awk '{print $2}')"

    # ç£ç›˜
    echo "  Disk: $(df -h / | tail -1 | awk '{print $2}')"

    # GPU
    if lspci | grep -qi nvidia; then
        echo "  GPU: $(lspci | grep -i nvidia | head -1 | cut -d: -f3)"
    else
        echo "  âœ— No NVIDIA GPU detected"
        return 1
    fi
}

# 2. æ£€æŸ¥æ“ä½œç³»ç»Ÿ
check_os() {
    echo "2. æ“ä½œç³»ç»Ÿæ£€æŸ¥"

    . /etc/os-release
    echo "  OS: $PRETTY_NAME"
    echo "  Kernel: $(uname -r)"

    # æ£€æŸ¥æ”¯æŒçš„æ“ä½œç³»ç»Ÿ
    if [[ "$ID" != "ubuntu" && "$ID" != "centos" && "$ID" != "rhel" ]]; then
        echo "  âš  Unsupported OS: $ID"
    fi
}

# 3. æ£€æŸ¥ç½‘ç»œ
check_network() {
    echo "3. ç½‘ç»œæ£€æŸ¥"

    # Internet è®¿é—®
    if ping -c 1 8.8.8.8 &>/dev/null; then
        echo "  âœ“ Internet access"
    else
        echo "  âœ— No internet access"
        return 1
    fi

    # NVIDIA ä»“åº“è®¿é—®
    if curl -s https://developer.download.nvidia.com &>/dev/null; then
        echo "  âœ“ NVIDIA repository accessible"
    else
        echo "  âœ— NVIDIA repository not accessible"
    fi
}

# 4. æ£€æŸ¥å…ˆå†³æ¡ä»¶
check_prerequisites() {
    echo "4. å…ˆå†³æ¡ä»¶æ£€æŸ¥"

    local required_packages=("build-essential" "python3" "ansible")

    for pkg in "${required_packages[@]}"; do
        if dpkg -l | grep -q "^ii  $pkg"; then
            echo "  âœ“ $pkg installed"
        else
            echo "  âœ— $pkg not installed"
        fi
    done
}

# æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
check_hardware
check_os
check_network
check_prerequisites

echo ""
echo "=== æ£€æŸ¥å®Œæˆ ==="
```

#### æ­¥éª¤ 2: é…ç½® Ansible Inventory

```ini
# ansible/inventory/hosts

[all:vars]
ansible_user=ubuntu
ansible_become=true
ansible_python_interpreter=/usr/bin/python3

# GPU èŠ‚ç‚¹ç»„
[gpu_nodes]
gpu-01 ansible_host=192.168.1.101 gpu_type=A100
gpu-02 ansible_host=192.168.1.102 gpu_type=A100
gpu-03 ansible_host=192.168.1.103 gpu_type=H100

# ç¯å¢ƒåˆ†ç»„
[dev]
gpu-01

[staging]
gpu-02

[production]
gpu-03

# è§’è‰²åˆ†ç»„
[training_nodes]
gpu-[01:02]

[inference_nodes]
gpu-03
```

#### æ­¥éª¤ 3: æ‰§è¡Œéƒ¨ç½²

```bash
# åŸºç¡€éƒ¨ç½²ï¼ˆæ¨èæ–°æ‰‹ï¼‰
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml

# å®Œæ•´ä¼˜åŒ–éƒ¨ç½²ï¼ˆæ¨èç”Ÿäº§ï¼‰
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml

# è‡ªå®šä¹‰éƒ¨ç½²
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml \
  -e "driver_installation_method=precompiled" \
  -e "enable_cpu_optimization=true" \
  -e "validation_level=full" \
  --limit production
```

---

## é©±åŠ¨å®‰è£…æœ€ä½³å®è·µ

### æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘

```
å¼€å§‹
  â”‚
  â”œâ”€ æ˜¯å¦ä½¿ç”¨ Kubernetes?
  â”‚  â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ Driver Container
  â”‚  â””â”€ å¦ â†’ ç»§ç»­
  â”‚
  â”œâ”€ é›†ç¾¤è§„æ¨¡?
  â”‚  â”œâ”€ > 50 èŠ‚ç‚¹ â†’ ä½¿ç”¨ Precompiled
  â”‚  â”œâ”€ 10-50 èŠ‚ç‚¹ â†’ è€ƒè™‘ Precompiled
  â”‚  â””â”€ < 10 èŠ‚ç‚¹ â†’ Native æˆ– Precompiled
  â”‚
  â”œâ”€ å†…æ ¸ç‰ˆæœ¬ç»Ÿä¸€?
  â”‚  â”œâ”€ æ˜¯ â†’ Precompiledï¼ˆå¼ºçƒˆæ¨èï¼‰
  â”‚  â””â”€ å¦ â†’ Native æˆ–ä¸ºæ¯ä¸ªç‰ˆæœ¬æ„å»º
  â”‚
  â”œâ”€ æ˜¯å¦éœ€è¦å¿«é€Ÿå›æ»š?
  â”‚  â”œâ”€ æ˜¯ â†’ Driver Container æˆ– Precompiled
  â”‚  â””â”€ å¦ â†’ Native
  â”‚
  â””â”€ æœ€ç»ˆå†³ç­–
```

### Native å®‰è£…æœ€ä½³å®è·µ

**é€‚ç”¨åœºæ™¯**:
- å°è§„æ¨¡éƒ¨ç½²ï¼ˆ< 10 èŠ‚ç‚¹ï¼‰
- ä¼ ç»Ÿæ•°æ®ä¸­å¿ƒ
- ä¸éœ€è¦é¢‘ç¹æ›´æ–°

**é…ç½®å»ºè®®**:

```yaml
# ansible/roles/gpu_baseline/defaults/main.yml
driver_installation_method: native
auto_detect_cuda_version: true
nvidia_driver_version: "535"  # ä¼šè¢«è‡ªåŠ¨æ£€æµ‹è¦†ç›–
cuda_version: "12-2"

# é‡å¯é…ç½®
nvidia_driver_skip_reboot: false
reboot_timeout: 600

# éªŒè¯
run_post_install_validation: true
```

**æ³¨æ„äº‹é¡¹**:

1. **ç¼–è¯‘æ—¶é—´**: é¢„ç•™ 20-30 åˆ†é’Ÿ/èŠ‚ç‚¹
2. **å†…æ ¸æ›´æ–°**: å†…æ ¸æ›´æ–°åéœ€é‡æ–°ç¼–è¯‘
3. **ä¾èµ–ç®¡ç†**: ç¡®ä¿ build-essential å’Œ kernel-headers å·²å®‰è£…

```bash
# é¢„å®‰è£…ä¾èµ–
ansible -i inventory/hosts gpu_nodes -m apt -a \
  "name=build-essential,linux-headers-$(uname -r) state=present"

# å®‰è£…é©±åŠ¨
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml

# éªŒè¯
ansible -i inventory/hosts gpu_nodes -m shell -a "nvidia-smi"
```

### Precompiled å®‰è£…æœ€ä½³å®è·µ

**é€‚ç”¨åœºæ™¯**:
- å¤§è§„æ¨¡éƒ¨ç½²ï¼ˆ> 50 èŠ‚ç‚¹ï¼‰
- éœ€è¦å¿«é€Ÿéƒ¨ç½²
- å†…æ ¸ç‰ˆæœ¬ç»Ÿä¸€æˆ–å¯æ§

**å®Œæ•´æµç¨‹**:

```bash
# 1. è¯†åˆ«æ‰€æœ‰å†…æ ¸ç‰ˆæœ¬
ansible -i inventory/hosts all -m shell \
  -a "uname -r" | grep -v ">>" | sort -u > kernels.txt

# 2. æ‰¹é‡æ„å»ºé¢„ç¼–è¯‘é©±åŠ¨
cat > build_config.sh << 'EOF'
#!/bin/bash
DRIVER_VERSIONS=("535.154.05" "550.90.07")
KERNEL_VERSIONS=($(cat kernels.txt))

export OUTPUT_DIR=/opt/precompiled-drivers
export USE_CONTAINER=true

./scripts/install/batch_build_drivers.sh
EOF

bash build_config.sh

# 3. å»ºç«‹é©±åŠ¨ä»“åº“
mkdir -p /var/www/drivers
cp -r /opt/precompiled-drivers/* /var/www/drivers/
./scripts/utils/manage_precompiled_drivers.sh update-index

# 4. å¯åŠ¨ HTTP æœåŠ¡
cd /var/www/drivers
python3 -m http.server 8080 &

# 5. æ‰¹é‡éƒ¨ç½²
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml \
  -e "driver_installation_method=precompiled" \
  -e "precompiled_repo=http://repo-server:8080" \
  --forks 20

# 6. éªŒè¯
ansible -i inventory/hosts all -m shell \
  -a "nvidia-smi && /usr/local/bin/gpu-benchmark verify"
```

**å…³é”®é…ç½®**:

```yaml
# é¢„ç¼–è¯‘é©±åŠ¨é…ç½®
driver_installation_method: precompiled
precompiled_repo: "http://internal-repo.company.com/drivers"
use_precompiled: true

# è‡ªåŠ¨é€‰æ‹©åŒ¹é…çš„é©±åŠ¨
auto_match_kernel: true

# å›æ»šé…ç½®
enable_driver_backup: true
backup_retention_days: 30
```

### Driver Container æœ€ä½³å®è·µ

**é€‚ç”¨åœºæ™¯**:
- Kubernetes é›†ç¾¤
- éœ€è¦ç‰ˆæœ¬éš”ç¦»
- éœ€è¦å¿«é€Ÿåˆ‡æ¢ç‰ˆæœ¬

**é…ç½®**:

```yaml
driver_installation_method: driver-container
driver_container_image: nvcr.io/nvidia/driver
driver_container_tag: 535.154.05-ubuntu22.04
driver_container_enable_persistence: true

# Health check é…ç½®
driver_health_check_interval: 300
driver_auto_restart: true
```

**ç®¡ç†å‘½ä»¤**:

```bash
# æŸ¥çœ‹é©±åŠ¨å®¹å™¨çŠ¶æ€
systemctl status nvidia-driver

# æŸ¥çœ‹æ—¥å¿—
journalctl -u nvidia-driver -f

# é‡å¯é©±åŠ¨å®¹å™¨
systemctl restart nvidia-driver

# åˆ‡æ¢é©±åŠ¨ç‰ˆæœ¬
# 1. åœæ­¢å½“å‰å®¹å™¨
systemctl stop nvidia-driver

# 2. æ›´æ–°é…ç½®
sudo sed -i 's/535.154.05/550.90.07/g' /etc/systemd/system/nvidia-driver.service

# 3. é‡æ–°åŠ è½½å¹¶å¯åŠ¨
systemctl daemon-reload
systemctl start nvidia-driver

# 4. éªŒè¯
nvidia-smi
```

---

## æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### CPU ä¼˜åŒ–é…ç½®

**æ¨èé…ç½®**ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰:

```yaml
# ansible/roles/cpu_optimization/defaults/main.yml

# CPU Governorï¼ˆæœ€é‡è¦ï¼‰
cpu_governor: performance  # å¼ºåˆ¶æ€§èƒ½æ¨¡å¼

# Turbo Boost
enable_turbo_boost: true  # Intel Turbo Boost / AMD Precision Boost

# NUMA ä¼˜åŒ–
optimize_numa: true
numa_balancing: false  # ç¦ç”¨è‡ªåŠ¨ NUMA å¹³è¡¡

# C-Statesï¼ˆé™ä½å»¶è¿Ÿï¼‰
c_states_config:
  max_cstate: 1  # é™åˆ¶åˆ° C1
  disable_deep_sleep: true

# IOMMU
kernel_params:
  intel_iommu: "on"
  iommu: "pt"  # passthrough æ¨¡å¼
  pcie_aspm: "off"  # ç¦ç”¨ PCIe ç”µæºç®¡ç†

# å†…å­˜ä¼˜åŒ–
vm_swappiness: 10  # å‡å°‘ swap ä½¿ç”¨
transparent_hugepages: madvise

# IRQ äº²å’Œæ€§
configure_irq_affinity: true
```

**éªŒè¯ä¼˜åŒ–æ•ˆæœ**:

```bash
# è¿è¡Œç³»ç»Ÿæ£€æŸ¥
./scripts/validation/system_check.sh

# é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š
# âœ“ CPU Governor: performance (on all cores)
# âœ“ Turbo Boost: enabled
# âœ“ NUMA nodes: 2
# âœ“ GPU 0 on NUMA node 0
# âœ“ C-States: C1 only
# âœ“ IOMMU: enabled (passthrough mode)

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
# ä¼˜åŒ–å‰
./scripts/benchmarks/nccl_benchmark.sh > before.txt

# åº”ç”¨ä¼˜åŒ–
ansible-playbook -i inventory/hosts playbooks/apply_cpu_optimization.yml

# ä¼˜åŒ–å
./scripts/benchmarks/nccl_benchmark.sh > after.txt

# å¯¹æ¯”ç»“æœ
diff before.txt after.txt
```

**é¢„æœŸæ€§èƒ½æå‡**:

| å·¥ä½œè´Ÿè½½ç±»å‹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------------|--------|--------|------|
| NCCL AllReduce | 180 GB/s | 245 GB/s | 36% |
| è®­ç»ƒååé‡ | 2500 samples/s | 3200 samples/s | 28% |
| GPU åˆ©ç”¨ç‡ | 75% | 92% | 23% |

### NUMA é…ç½®æœ€ä½³å®è·µ

**æ£€æŸ¥ NUMA æ‹“æ‰‘**:

```bash
# æŸ¥çœ‹ NUMA èŠ‚ç‚¹
numactl --hardware

# æŸ¥çœ‹ GPU-NUMA æ˜ å°„
nvidia-smi topo -m

# æ£€æŸ¥ GPU äº²å’Œæ€§
for gpu in $(seq 0 7); do
  echo "GPU $gpu: NUMA node $(cat /sys/class/drm/card$gpu/device/numa_node)"
done
```

**æ­£ç¡®çš„ NUMA ç»‘å®š**:

```bash
#!/bin/bash
# è¿è¡Œè®­ç»ƒæ—¶ç»‘å®šåˆ°æ­£ç¡®çš„ NUMA èŠ‚ç‚¹

GPU_ID=0
NUMA_NODE=$(cat /sys/class/drm/card${GPU_ID}/device/numa_node)

# å¯åŠ¨è®­ç»ƒï¼Œç»‘å®šåˆ°å¯¹åº”çš„ NUMA èŠ‚ç‚¹
numactl --cpunodebind=${NUMA_NODE} --membind=${NUMA_NODE} \
  python train.py --gpu ${GPU_ID}
```

### ç½‘ç»œä¼˜åŒ–ï¼ˆå¤šèŠ‚ç‚¹è®­ç»ƒï¼‰

**InfiniBand ä¼˜åŒ–**:

```bash
# /etc/modprobe.d/mlx5_core.conf
options mlx5_core log_max_qp=20

# ä¼˜åŒ– IB å‚æ•°
echo 1 > /sys/class/net/ib0/mode
echo 65520 > /sys/class/net/ib0/mtu

# éªŒè¯
ibstat
ibstatus
```

**RoCE ä¼˜åŒ–**:

```bash
# å¯ç”¨ PFC (Priority Flow Control)
mlnx_qos -i ens1f0 --pfc 0,0,0,1,0,0,0,0

# è®¾ç½® ECN
mlnx_qos -i ens1f0 --trust dscp

# é…ç½® DSCP
echo 4 > /sys/class/infiniband/mlx5_0/tc/1/traffic_class
```

---

## éªŒè¯å’Œæµ‹è¯•æœ€ä½³å®è·µ

### ä¸‰çº§éªŒè¯ç­–ç•¥

#### Level 1: å¿«é€ŸéªŒè¯ï¼ˆ5 åˆ†é’Ÿï¼‰

**ç›®çš„**: ç¡®è®¤åŸºæœ¬åŠŸèƒ½æ­£å¸¸

```bash
# è¿è¡Œå¿«é€Ÿæ£€æŸ¥
./scripts/validation/quick_check.sh

# æˆ–é€šè¿‡ Ansible
ansible -i inventory/hosts gpu_nodes -m script \
  -a "./scripts/validation/quick_check.sh"
```

**æ£€æŸ¥é¡¹**:
- âœ“ nvidia-smi å¯ç”¨
- âœ“ GPU æ£€æµ‹
- âœ“ é©±åŠ¨ç‰ˆæœ¬
- âœ“ æ¸©åº¦æ­£å¸¸
- âœ“ æ—  ECC é”™è¯¯

#### Level 2: æ ‡å‡†éªŒè¯ï¼ˆ15 åˆ†é’Ÿï¼‰

**ç›®çš„**: å…¨é¢ç³»ç»Ÿæ£€æŸ¥

```bash
# è¿è¡Œç³»ç»Ÿæ£€æŸ¥
./scripts/validation/system_check.sh

# ç”Ÿæˆ JSON æŠ¥å‘Š
./scripts/validation/system_check.sh --json > system_report.json
```

**æ£€æŸ¥é¡¹**ï¼ˆ8 å¤§ç±»ï¼‰:
1. CPU é…ç½®
2. NUMA é…ç½®
3. IOMMU é…ç½®
4. PCIe é…ç½®
5. GPU é…ç½®
6. å†…å­˜é…ç½®
7. å†…æ ¸å‚æ•°
8. å®¹å™¨è¿è¡Œæ—¶

#### Level 3: å®Œæ•´éªŒè¯ï¼ˆ30-60 åˆ†é’Ÿï¼‰

**ç›®çš„**: æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# 1. å¸¦å®½æµ‹è¯•
./scripts/validation/bandwidth_test.sh

# 2. NCCL æµ‹è¯•
./scripts/benchmarks/nccl_benchmark.sh

# 3. è®­ç»ƒåŸºå‡†ï¼ˆå¯é€‰ï¼‰
MODEL_SIZE=GPT-1.2B ./scripts/benchmarks/megatron_benchmark.sh

# 4. å‹åŠ›æµ‹è¯•
./scripts/validation/stress_test.sh --duration 3600  # 1 å°æ—¶
```

### è‡ªåŠ¨åŒ–éªŒè¯æµç¨‹

```yaml
# playbooks/comprehensive_validation.yml

- name: Comprehensive GPU Cluster Validation
  hosts: gpu_nodes
  become: yes
  serial: "20%"  # æ¯æ¬¡ 20% èŠ‚ç‚¹

  tasks:
    - name: Level 1 - Quick Check
      script: ../scripts/validation/quick_check.sh
      register: quick_check

    - name: Level 2 - System Check
      script: ../scripts/validation/system_check.sh --json
      register: system_check

    - name: Save reports
      copy:
        content: "{{ system_check.stdout }}"
        dest: "/var/log/validation_{{ inventory_hostname }}_{{ ansible_date_time.epoch }}.json"

    - name: Level 3 - Bandwidth Test
      script: ../scripts/validation/bandwidth_test.sh
      when: validation_level == "full"

    - name: Level 3 - NCCL Benchmark
      script: ../scripts/benchmarks/nccl_benchmark.sh
      when: validation_level == "full"
```

**è¿è¡ŒéªŒè¯**:

```bash
# å¿«é€ŸéªŒè¯
ansible-playbook -i inventory/hosts playbooks/comprehensive_validation.yml \
  -e "validation_level=quick"

# å®Œæ•´éªŒè¯
ansible-playbook -i inventory/hosts playbooks/comprehensive_validation.yml \
  -e "validation_level=full"

# æ”¶é›†æŠ¥å‘Š
ansible -i inventory/hosts all -m fetch \
  -a "src=/var/log/validation_*.json dest=./reports/"
```

### åŸºå‡†æµ‹è¯•åŸºçº¿å»ºç«‹

**å»ºç«‹é›†ç¾¤åŸºçº¿**:

```bash
#!/bin/bash
# establish_baseline.sh

CLUSTER_NAME="production"
BASELINE_DIR="/opt/baselines/${CLUSTER_NAME}"

mkdir -p "${BASELINE_DIR}"

# 1. æ”¶é›†ç³»ç»Ÿä¿¡æ¯
for node in $(ansible -i inventory/hosts gpu_nodes --list-hosts | grep -v hosts); do
  echo "Collecting baseline from $node..."

  # ç³»ç»Ÿé…ç½®
  ssh $node "nvidia-smi -q" > "${BASELINE_DIR}/${node}_gpu_info.txt"

  # å¸¦å®½æµ‹è¯•
  ssh $node "./scripts/validation/bandwidth_test.sh" > "${BASELINE_DIR}/${node}_bandwidth.json"

  # NCCL æµ‹è¯•
  ssh $node "./scripts/benchmarks/nccl_benchmark.sh" > "${BASELINE_DIR}/${node}_nccl.json"
done

# 2. ç”ŸæˆåŸºçº¿æŠ¥å‘Š
python3 << EOF
import json
import glob

baselines = {}
for file in glob.glob("${BASELINE_DIR}/*_bandwidth.json"):
    node = file.split('/')[-1].split('_')[0]
    with open(file) as f:
        baselines[node] = json.load(f)

with open("${BASELINE_DIR}/cluster_baseline.json", 'w') as f:
    json.dump(baselines, f, indent=2)

print(f"Baseline established for {len(baselines)} nodes")
EOF

echo "Baseline saved to: ${BASELINE_DIR}"
```

**ä½¿ç”¨åŸºçº¿å¯¹æ¯”**:

```bash
# è¿è¡Œå½“å‰æµ‹è¯•
./scripts/validation/bandwidth_test.sh > current_test.json

# å¯¹æ¯”åŸºçº¿
python3 << EOF
import json

with open('current_test.json') as f:
    current = json.load(f)

with open('/opt/baselines/production/gpu-01_bandwidth.json') as f:
    baseline = json.load(f)

# å¯¹æ¯”å…³é”®æŒ‡æ ‡
for metric in ['pcie_bandwidth', 'nvlink_bandwidth']:
    current_val = current.get(metric)
    baseline_val = baseline.get(metric)

    if current_val and baseline_val:
        diff = (current_val - baseline_val) / baseline_val * 100
        status = "âœ“" if abs(diff) < 5 else "âš "
        print(f"{status} {metric}: {current_val:.2f} GB/s (baseline: {baseline_val:.2f}, diff: {diff:+.1f}%)")
EOF
```

---

## ç›‘æ§å’Œç»´æŠ¤

### ç›‘æ§æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Monitoring Stack                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  DCGM    â”‚â†’ â”‚Prometheusâ”‚â†’ â”‚ Grafana  â”‚     â”‚
â”‚  â”‚ Exporter â”‚  â”‚          â”‚  â”‚          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â†‘                            â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ GPU Metrics  â”‚          â”‚ Alertmanagerâ”‚    â”‚
â”‚  â”‚ - Utilizationâ”‚          â”‚ - Slack     â”‚    â”‚
â”‚  â”‚ - Temperatureâ”‚          â”‚ - Email     â”‚    â”‚
â”‚  â”‚ - Memory     â”‚          â”‚ - PagerDuty â”‚    â”‚
â”‚  â”‚ - Power      â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”‚ - ECC Errors â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ç›‘æ§æŒ‡æ ‡

**GPU æŒ‡æ ‡**:

```yaml
# å¿…é¡»ç›‘æ§çš„æŒ‡æ ‡
gpu_metrics:
  - gpu_utilization        # > 80% (è®­ç»ƒæ—¶)
  - memory_utilization     # < 95%
  - temperature           # < 85Â°C
  - power_usage           # ç¬¦åˆé¢„æœŸ
  - ecc_errors            # = 0
  - pcie_throughput       # ç¬¦åˆåŸºçº¿
  - sm_clock              # ç¬¦åˆé¢„æœŸ
  - memory_clock          # ç¬¦åˆé¢„æœŸ

# å‘Šè­¦é˜ˆå€¼
alerts:
  high_temperature:
    threshold: 85
    severity: warning
  critical_temperature:
    threshold: 90
    severity: critical
  ecc_errors:
    threshold: 1
    severity: critical
  low_utilization:
    threshold: 20
    duration: 1h
    severity: info
```

### DCGM ç›‘æ§éƒ¨ç½²

```bash
# 1. å®‰è£… DCGM
ansible -i inventory/hosts gpu_nodes -m apt \
  -a "name=datacenter-gpu-manager state=present"

# 2. å¯åŠ¨ DCGM
ansible -i inventory/hosts gpu_nodes -m service \
  -a "name=nvidia-dcgm state=started enabled=yes"

# 3. éƒ¨ç½² DCGM Exporter
cat > dcgm-exporter.yaml << EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: dcgm-exporter
  namespace: gpu-monitoring
spec:
  selector:
    matchLabels:
      app: dcgm-exporter
  template:
    metadata:
      labels:
        app: dcgm-exporter
    spec:
      containers:
      - name: dcgm-exporter
        image: nvcr.io/nvidia/k8s/dcgm-exporter:3.1.7-3.1.4-ubuntu22.04
        ports:
        - containerPort: 9400
          name: metrics
        securityContext:
          privileged: true
        volumeMounts:
        - name: pod-resources
          mountPath: /var/lib/kubelet/pod-resources
      volumes:
      - name: pod-resources
        hostPath:
          path: /var/lib/kubelet/pod-resources
EOF

kubectl apply -f dcgm-exporter.yaml

# 4. é…ç½® Prometheus æŠ“å–
cat >> prometheus.yml << EOF
scrape_configs:
  - job_name: 'dcgm'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_label_app]
      action: keep
      regex: dcgm-exporter
EOF
```

### Grafana ä»ªè¡¨æ¿

**æ¨èä»ªè¡¨æ¿**:

1. **NVIDIA DCGM Exporter Dashboard** (ID: 12239)
2. **GPU Cluster Dashboard** (è‡ªå®šä¹‰)

```json
{
  "dashboard": {
    "title": "GPU Cluster Overview",
    "panels": [
      {
        "title": "GPU Utilization",
        "targets": [{
          "expr": "DCGM_FI_DEV_GPU_UTIL"
        }]
      },
      {
        "title": "GPU Temperature",
        "targets": [{
          "expr": "DCGM_FI_DEV_GPU_TEMP"
        }]
      },
      {
        "title": "GPU Memory Usage",
        "targets": [{
          "expr": "DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE * 100"
        }]
      },
      {
        "title": "ECC Errors",
        "targets": [{
          "expr": "DCGM_FI_DEV_ECC_DBE_VOL_TOTAL"
        }]
      }
    ]
  }
}
```

### æ—¥å¸¸ç»´æŠ¤æ£€æŸ¥æ¸…å•

**æ¯æ—¥æ£€æŸ¥**ï¼ˆè‡ªåŠ¨åŒ–ï¼‰:

```bash
#!/bin/bash
# daily_health_check.sh

# 1. GPU å¥åº·æ£€æŸ¥
nvidia-smi --query-gpu=index,name,driver_version,temperature.gpu,utilization.gpu,memory.used,memory.total,ecc.errors.uncorrected.aggregate.total \
  --format=csv,noheader

# 2. é©±åŠ¨çŠ¶æ€
if ! nvidia-smi &>/dev/null; then
  echo "ALERT: nvidia-smi failed"
  exit 1
fi

# 3. æ¸©åº¦æ£€æŸ¥
MAX_TEMP=$(nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader | sort -n | tail -1)
if [ "$MAX_TEMP" -gt 85 ]; then
  echo "ALERT: High temperature detected: ${MAX_TEMP}Â°C"
fi

# 4. ECC é”™è¯¯æ£€æŸ¥
ECC_ERRORS=$(nvidia-smi --query-gpu=ecc.errors.uncorrected.aggregate.total --format=csv,noheader | awk '{s+=$1} END {print s}')
if [ "$ECC_ERRORS" -gt 0 ]; then
  echo "ALERT: ECC errors detected: $ECC_ERRORS"
fi

# 5. PCIe æ£€æŸ¥
./scripts/validation/bandwidth_test.sh --quick

# 6. ç£ç›˜ç©ºé—´
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | tr -d '%')
if [ "$DISK_USAGE" -gt 80 ]; then
  echo "WARNING: Disk usage high: ${DISK_USAGE}%"
fi
```

**æ¯å‘¨æ£€æŸ¥**ï¼ˆæ‰‹åŠ¨æˆ–è‡ªåŠ¨ï¼‰:

```bash
#!/bin/bash
# weekly_maintenance.sh

# 1. å®Œæ•´ç³»ç»ŸéªŒè¯
./scripts/validation/system_check.sh --json > weekly_report_$(date +%Y%m%d).json

# 2. æ€§èƒ½åŸºçº¿å¯¹æ¯”
./scripts/benchmarks/nccl_benchmark.sh > nccl_weekly.txt
diff nccl_baseline.txt nccl_weekly.txt

# 3. é©±åŠ¨æ—¥å¿—æ£€æŸ¥
dmesg | grep -i nvidia | grep -i error

# 4. åŒ…æ›´æ–°æ£€æŸ¥
apt list --upgradable | grep nvidia

# 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find /tmp -name "nvidia*" -mtime +7 -delete
find /var/log -name "nvidia*" -mtime +30 -delete
```

**æ¯æœˆæ£€æŸ¥**:

- å®Œæ•´åŸºå‡†æµ‹è¯•
- é©±åŠ¨ç‰ˆæœ¬è¯„ä¼°
- å®‰å…¨æ›´æ–°
- å®¹é‡è§„åˆ’
- æ–‡æ¡£æ›´æ–°

---

## å®‰å…¨æœ€ä½³å®è·µ

### è®¿é—®æ§åˆ¶

**ç”¨æˆ·æƒé™ç®¡ç†**:

```bash
# åˆ›å»º GPU ç”¨æˆ·ç»„
groupadd gpuusers

# æ·»åŠ ç”¨æˆ·åˆ°ç»„
usermod -aG gpuusers alice
usermod -aG gpuusers bob

# é…ç½® nvidia-smi æƒé™
cat > /etc/udev/rules.d/99-nvidia.rules << EOF
KERNEL=="nvidia*", GROUP="gpuusers", MODE="0660"
KERNEL=="nvidiactl", GROUP="gpuusers", MODE="0660"
KERNEL=="nvidia-modeset", GROUP="gpuusers", MODE="0660"
KERNEL=="nvidia-uvm", GROUP="gpuusers", MODE="0660"
EOF

# é‡æ–°åŠ è½½ udev
udevadm control --reload-rules
udevadm trigger
```

**å®¹å™¨éš”ç¦»**:

```bash
# é™åˆ¶å®¹å™¨å¯è§çš„ GPU
docker run --gpus '"device=0,1"' ...  # åªèƒ½è®¿é—® GPU 0 å’Œ 1

# ä½¿ç”¨ MIG (Multi-Instance GPU) è¿›è¡Œç¡¬ä»¶éš”ç¦»
nvidia-smi mig -cgi 19,19,19 -C  # åˆ›å»º 3 ä¸ª MIG å®ä¾‹
```

### å®‰å…¨åŠ å›º

**ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½**:

```bash
# /etc/modprobe.d/nvidia-security.conf

# ç¦ç”¨ persistence modeï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
# options nvidia NVreg_EnablePersistenced=0

# å¯ç”¨å®‰å…¨æ¨¡å¼
options nvidia NVreg_EnableGpuFirmware=1

# ç¦ç”¨è°ƒè¯•
options nvidia NVreg_EnableDbgBreakpoint=0
```

**å®¡è®¡æ—¥å¿—**:

```bash
# å¯ç”¨å®¡è®¡
auditctl -w /dev/nvidia0 -p wa -k gpu_access
auditctl -w /usr/bin/nvidia-smi -p x -k gpu_tools

# æŸ¥çœ‹å®¡è®¡æ—¥å¿—
ausearch -k gpu_access
```

### ç½‘ç»œå®‰å…¨

**é˜²ç«å¢™é…ç½®**:

```bash
# UFW é…ç½®ï¼ˆUbuntuï¼‰
# åªå…è®¸å†…éƒ¨ç½‘ç»œè®¿é—® NCCL ç«¯å£
ufw allow from 192.168.0.0/16 to any port 50000:51000 proto tcp

# å…è®¸ Prometheus æŠ“å–
ufw allow from monitoring-server to any port 9400 proto tcp
```

**SSH åŠ å›º**:

```bash
# /etc/ssh/sshd_config
PermitRootLogin no
PasswordAuthentication no
PubkeyAuthentication yes
AllowUsers gpu-admin@192.168.0.0/16
```

---

## æ•…éšœæ’é™¤æµç¨‹

### æ•…éšœåˆ†ç±»å’Œå¤„ç†æµç¨‹

```
æ•…éšœæŠ¥å‘Š
    â”‚
    â”œâ”€ é©±åŠ¨é—®é¢˜ï¼Ÿ
    â”‚  â”œâ”€ nvidia-smi å¤±è´¥ â†’ æ£€æŸ¥é©±åŠ¨åŠ è½½ â†’ é‡å¯é©±åŠ¨/é‡æ–°å®‰è£…
    â”‚  â”œâ”€ ç‰ˆæœ¬ä¸åŒ¹é… â†’ æ£€æŸ¥ CUDA å…¼å®¹æ€§ â†’ æ›´æ–°é©±åŠ¨
    â”‚  â””â”€ æ¨¡å—åŠ è½½å¤±è´¥ â†’ æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ â†’ é‡æ–°ç¼–è¯‘
    â”‚
    â”œâ”€ æ€§èƒ½é—®é¢˜ï¼Ÿ
    â”‚  â”œâ”€ GPU åˆ©ç”¨ç‡ä½ â†’ æ£€æŸ¥ CPU/NUMA â†’ ä¼˜åŒ–é…ç½®
    â”‚  â”œâ”€ å¸¦å®½ä½ â†’ è¿è¡Œ bandwidth_test â†’ æ£€æŸ¥ PCIe/NVLink
    â”‚  â””â”€ è®­ç»ƒæ…¢ â†’ è¿è¡Œ NCCL æµ‹è¯• â†’ æ£€æŸ¥ç½‘ç»œ
    â”‚
    â”œâ”€ ç¡¬ä»¶é—®é¢˜ï¼Ÿ
    â”‚  â”œâ”€ é«˜æ¸© â†’ æ£€æŸ¥æ•£çƒ­ â†’ æ¸…ç†/RMA
    â”‚  â”œâ”€ ECC é”™è¯¯ â†’ æ£€æŸ¥æ—¥å¿— â†’ éš”ç¦»/RMA
    â”‚  â””â”€ GPU æ‰çº¿ â†’ æ£€æŸ¥ PCIe â†’ é‡æ–°æ’æ‹”/RMA
    â”‚
    â””â”€ å…¶ä»–é—®é¢˜ï¼Ÿ
       â”œâ”€ å®¹å™¨è®¿é—®å¤±è´¥ â†’ æ£€æŸ¥ runtime â†’ é‡æ–°é…ç½®
       â”œâ”€ å¤šèŠ‚ç‚¹é€šä¿¡å¤±è´¥ â†’ æ£€æŸ¥ç½‘ç»œ â†’ é…ç½® RDMA
       â””â”€ æƒé™é—®é¢˜ â†’ æ£€æŸ¥ç”¨æˆ·ç»„ â†’ è°ƒæ•´æƒé™
```

### å¸¸è§é—®é¢˜å¿«é€Ÿè§£å†³

#### é—®é¢˜ 1: nvidia-smi æ— å“åº”

```bash
# è¯Šæ–­
sudo dmesg | grep -i nvidia | tail -20
lsmod | grep nvidia
ls -la /dev/nvidia*

# è§£å†³æ–¹æ¡ˆ 1: é‡æ–°åŠ è½½æ¨¡å—
sudo rmmod nvidia_drm nvidia_modeset nvidia_uvm nvidia
sudo modprobe nvidia

# è§£å†³æ–¹æ¡ˆ 2: é‡å¯é©±åŠ¨æœåŠ¡ï¼ˆDriver Containerï¼‰
sudo systemctl restart nvidia-driver

# è§£å†³æ–¹æ¡ˆ 3: é‡æ–°å®‰è£…é©±åŠ¨
./scripts/utils/manage_precompiled_drivers.sh rollback
```

#### é—®é¢˜ 2: GPU æ¸©åº¦è¿‡é«˜

```bash
# æ£€æŸ¥æ¸©åº¦
nvidia-smi --query-gpu=temperature.gpu,temperature.memory --format=csv

# æ£€æŸ¥é£æ‰‡
nvidia-smi --query-gpu=fan.speed --format=csv

# ä¸´æ—¶é™ä½åŠŸç‡é™åˆ¶
sudo nvidia-smi -pl 250  # è®¾ç½®ä¸º 250W

# æ£€æŸ¥æ•°æ®ä¸­å¿ƒç¯å¢ƒ
sensors  # æ£€æŸ¥æœåŠ¡å™¨æ¸©åº¦
```

#### é—®é¢˜ 3: æ€§èƒ½ä¸‹é™

```bash
# 1. è¿è¡Œå®Œæ•´è¯Šæ–­
./scripts/validation/system_check.sh > diagnostic.txt

# 2. å¯¹æ¯”åŸºçº¿
./scripts/validation/bandwidth_test.sh > current_bandwidth.json
diff baseline_bandwidth.json current_bandwidth.json

# 3. æ£€æŸ¥ CPU é…ç½®
cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
cat /sys/devices/system/cpu/intel_pstate/no_turbo

# 4. æ£€æŸ¥ NUMA
numactl --hardware
nvidia-smi topo -m

# 5. é‡æ–°åº”ç”¨ä¼˜åŒ–
ansible-playbook -i inventory/hosts playbooks/apply_cpu_optimization.yml
```

### æ•…éšœæ’é™¤å·¥å…·åŒ…

```bash
#!/bin/bash
# gpu_troubleshoot.sh - ä¸€é”®æ•…éšœè¯Šæ–­

echo "=== GPU Troubleshooting Toolkit ==="
echo ""

# 1. åŸºç¡€æ£€æŸ¥
echo "1. Basic Checks"
echo "  Driver: $(nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -1)"
echo "  CUDA: $(nvcc --version 2>/dev/null | grep release | awk '{print $5}')"
echo "  GPUs: $(nvidia-smi --query-gpu=count --format=csv,noheader | head -1)"

# 2. æ¨¡å—æ£€æŸ¥
echo ""
echo "2. Kernel Modules"
lsmod | grep nvidia

# 3. è®¾å¤‡æ–‡ä»¶
echo ""
echo "3. Device Nodes"
ls -la /dev/nvidia* 2>/dev/null || echo "  No device nodes found"

# 4. PCIe æ£€æŸ¥
echo ""
echo "4. PCIe Status"
nvidia-smi --query-gpu=pcie.link.gen.current,pcie.link.width.current --format=csv

# 5. æ¸©åº¦å’ŒåŠŸç‡
echo ""
echo "5. Temperature and Power"
nvidia-smi --query-gpu=temperature.gpu,power.draw,power.limit --format=csv

# 6. ECC é”™è¯¯
echo ""
echo "6. ECC Errors"
nvidia-smi --query-gpu=ecc.errors.uncorrected.aggregate.total --format=csv

# 7. è¿›ç¨‹åˆ—è¡¨
echo ""
echo "7. GPU Processes"
nvidia-smi pmon -c 1

# 8. æ‹“æ‰‘
echo ""
echo "8. GPU Topology"
nvidia-smi topo -m

# 9. ç³»ç»Ÿæ—¥å¿—
echo ""
echo "9. Recent Errors (dmesg)"
dmesg | grep -i nvidia | grep -i error | tail -10

# 10. å»ºè®®
echo ""
echo "10. Recommendations"

# æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
DRIVER_VER=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -1)
if [[ "$DRIVER_VER" < "535" ]]; then
    echo "  âš  Consider upgrading driver to 535+ for better performance"
fi

# æ£€æŸ¥æ¸©åº¦
MAX_TEMP=$(nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader | sort -n | tail -1)
if [ "$MAX_TEMP" -gt 80 ]; then
    echo "  âš  High temperature detected: ${MAX_TEMP}Â°C"
    echo "    - Check cooling system"
    echo "    - Consider reducing power limit"
fi

# æ£€æŸ¥ ECC
ECC=$(nvidia-smi --query-gpu=ecc.errors.uncorrected.aggregate.total --format=csv,noheader | awk '{s+=$1} END {print s}')
if [ "$ECC" -gt 0 ]; then
    echo "  âœ— ECC errors detected: $ECC"
    echo "    - Run memory test"
    echo "    - Contact vendor if errors persist"
fi

echo ""
echo "=== Diagnostic Complete ==="
```

---

## ç‰ˆæœ¬ç®¡ç†å’Œå‡çº§

### é©±åŠ¨å‡çº§ç­–ç•¥

**é‡‘ä¸é›€å‡çº§æµç¨‹**:

```bash
#!/bin/bash
# canary_upgrade.sh

CANARY_NODES="gpu-test-01"
BLUE_NODES="gpu-prod-[01-10]"
GREEN_NODES="gpu-prod-[11-20]"

NEW_DRIVER="550.90.07"
VALIDATION_TIME=3600  # 1å°æ—¶

# 1. é‡‘ä¸é›€éƒ¨ç½²
echo "Phase 1: Canary Deployment"
ansible-playbook -i inventory/hosts playbooks/upgrade_driver.yml \
  -e "driver_version=${NEW_DRIVER}" \
  --limit "${CANARY_NODES}"

# 2. é‡‘ä¸é›€éªŒè¯
echo "Phase 2: Canary Validation (${VALIDATION_TIME}s)"
for i in $(seq 1 12); do
  ansible -i inventory/hosts canary -m script \
    -a "./scripts/validation/quick_check.sh"

  # æ£€æŸ¥ç›‘æ§æŒ‡æ ‡
  curl "http://prometheus:9090/api/v1/query?query=DCGM_FI_DEV_GPU_TEMP" | \
    jq '.data.result[] | select(.metric.instance == "gpu-test-01")'

  sleep 300
done

# 3. ç”¨æˆ·ç¡®è®¤
read -p "Canary successful? Continue to production? (yes/no) " -r
if [[ ! $REPLY =~ ^[Yy]es$ ]]; then
    echo "Upgrade cancelled"
    exit 1
fi

# 4. è“ç»„å‡çº§
echo "Phase 3: Blue Group Upgrade"
ansible-playbook -i inventory/hosts playbooks/upgrade_driver.yml \
  -e "driver_version=${NEW_DRIVER}" \
  --limit "${BLUE_NODES}" \
  --serial 5  # æ¯æ¬¡ 5 å°

# 5. éªŒè¯è“ç»„
ansible -i inventory/hosts blue -m script \
  -a "./scripts/validation/system_check.sh"

# 6. ç»¿ç»„å‡çº§
echo "Phase 4: Green Group Upgrade"
ansible-playbook -i inventory/hosts playbooks/upgrade_driver.yml \
  -e "driver_version=${NEW_DRIVER}" \
  --limit "${GREEN_NODES}" \
  --serial 5

# 7. å…¨é›†ç¾¤éªŒè¯
echo "Phase 5: Full Cluster Validation"
ansible-playbook -i inventory/hosts playbooks/comprehensive_validation.yml

echo "Upgrade Complete!"
```

### å›æ»šæµç¨‹

```bash
# æ–¹æ³• 1: ä½¿ç”¨ç®¡ç†è„šæœ¬ï¼ˆé¢„ç¼–è¯‘é©±åŠ¨ï¼‰
./scripts/utils/manage_precompiled_drivers.sh rollback

# æ–¹æ³• 2: ä½¿ç”¨ Ansible
ansible-playbook -i inventory/hosts playbooks/rollback_driver.yml

# æ–¹æ³• 3: æ‰‹åŠ¨å›æ»šï¼ˆDriver Containerï¼‰
systemctl stop nvidia-driver
# ä¿®æ”¹ /etc/systemd/system/nvidia-driver.service ä¸­çš„ç‰ˆæœ¬
systemctl daemon-reload
systemctl start nvidia-driver
```

### å˜æ›´ç®¡ç†æ¸…å•

**å‡çº§å‰**:
- [ ] å¤‡ä»½å½“å‰é…ç½®
- [ ] è®°å½•å½“å‰é©±åŠ¨ç‰ˆæœ¬
- [ ] è¿è¡ŒåŸºçº¿æµ‹è¯•
- [ ] é€šçŸ¥ç”¨æˆ·è®¡åˆ’ç»´æŠ¤çª—å£
- [ ] å‡†å¤‡å›æ»šæ–¹æ¡ˆ

**å‡çº§ä¸­**:
- [ ] éµå¾ªé‡‘ä¸é›€æµç¨‹
- [ ] å®æ—¶ç›‘æ§å…³é”®æŒ‡æ ‡
- [ ] è®°å½•æ‰€æœ‰æ“ä½œ
- [ ] ä¿æŒé€šä¿¡ç•…é€š

**å‡çº§å**:
- [ ] éªŒè¯æ‰€æœ‰èŠ‚ç‚¹
- [ ] è¿è¡Œæ€§èƒ½æµ‹è¯•
- [ ] å¯¹æ¯”åŸºçº¿
- [ ] æ›´æ–°æ–‡æ¡£
- [ ] é€šçŸ¥ç”¨æˆ·å®Œæˆ

---

## æ–‡æ¡£å’Œè®°å½•

### æ–‡æ¡£ç»“æ„

```
/opt/gpu-cluster-docs/
â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ hardware_inventory.xlsx      # ç¡¬ä»¶æ¸…å•
â”‚   â”œâ”€â”€ software_versions.md         # è½¯ä»¶ç‰ˆæœ¬
â”‚   â””â”€â”€ network_topology.pdf         # ç½‘ç»œæ‹“æ‰‘
â”œâ”€â”€ configurations/
â”‚   â”œâ”€â”€ driver_config.yaml           # é©±åŠ¨é…ç½®
â”‚   â”œâ”€â”€ optimization_params.yaml     # ä¼˜åŒ–å‚æ•°
â”‚   â””â”€â”€ monitoring_config.yaml       # ç›‘æ§é…ç½®
â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ performance_baseline.json    # æ€§èƒ½åŸºçº¿
â”‚   â”œâ”€â”€ bandwidth_baseline.json      # å¸¦å®½åŸºçº¿
â”‚   â””â”€â”€ training_baseline.json       # è®­ç»ƒåŸºçº¿
â”œâ”€â”€ procedures/
â”‚   â”œâ”€â”€ deployment_sop.md            # éƒ¨ç½² SOP
â”‚   â”œâ”€â”€ upgrade_procedure.md         # å‡çº§æµç¨‹
â”‚   â”œâ”€â”€ troubleshooting_guide.md     # æ•…éšœæ’é™¤
â”‚   â””â”€â”€ emergency_response.md        # åº”æ€¥å“åº”
â””â”€â”€ reports/
    â”œâ”€â”€ weekly_health_reports/       # å‘¨æŠ¥
    â”œâ”€â”€ monthly_performance/         # æœˆåº¦æ€§èƒ½
    â””â”€â”€ incident_reports/            # äº‹ä»¶æŠ¥å‘Š
```

### è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

```bash
#!/bin/bash
# generate_weekly_report.sh

REPORT_DATE=$(date +%Y%m%d)
REPORT_DIR="/opt/gpu-cluster-docs/reports/weekly_health_reports"
REPORT_FILE="${REPORT_DIR}/report_${REPORT_DATE}.md"

mkdir -p "${REPORT_DIR}"

cat > "${REPORT_FILE}" << EOF
# GPU Cluster Weekly Health Report
**Report Date**: $(date +%Y-%m-%d)
**Report Period**: $(date -d '7 days ago' +%Y-%m-%d) to $(date +%Y-%m-%d)

## Executive Summary

### Cluster Statistics
- Total Nodes: $(ansible -i inventory/hosts gpu_nodes --list-hosts | wc -l)
- Total GPUs: $(ansible -i inventory/hosts gpu_nodes -m shell -a "nvidia-smi --query-gpu=count --format=csv,noheader" | grep -v ">>" | awk '{s+=$1} END {print s}')
- Average GPU Utilization: TBD
- Average Temperature: TBD

### Alerts This Week
$(grep "$(date -d '7 days ago' +%Y-%m-%d)" /var/log/alerts.log | wc -l) alerts

## Node Status

EOF

# æ”¶é›†æ¯ä¸ªèŠ‚ç‚¹çŠ¶æ€
ansible -i inventory/hosts gpu_nodes -m script \
  -a "./scripts/validation/quick_check.sh" >> "${REPORT_FILE}"

# æ€§èƒ½æŒ‡æ ‡
cat >> "${REPORT_FILE}" << EOF

## Performance Metrics

### Bandwidth Tests
$(cat /tmp/weekly_bandwidth_results.txt)

### NCCL Performance
$(cat /tmp/weekly_nccl_results.txt)

## Issues and Resolutions

### Critical Issues
- None

### Warnings
$(grep WARN /var/log/system_check.log | tail -10)

## Maintenance Activities

### Completed
- Weekly health check
- Performance baseline validation

### Planned
- Monthly driver update review (next week)

## Recommendations

1. All systems operating within normal parameters
2. No immediate action required

---
*Report generated automatically by gpu-cluster-tools*
EOF

echo "Report generated: ${REPORT_FILE}"

# å‘é€æŠ¥å‘Š
mail -s "GPU Cluster Weekly Report - $(date +%Y-%m-%d)" \
  team@company.com < "${REPORT_FILE}"
```

---

## å›¢é˜Ÿåä½œ

### è§’è‰²å’ŒèŒè´£

**GPU é›†ç¾¤ç®¡ç†å‘˜**:
- éƒ¨ç½²å’Œç»´æŠ¤ GPU åŸºç¡€è®¾æ–½
- é©±åŠ¨å’Œè½¯ä»¶æ›´æ–°
- æ€§èƒ½ä¼˜åŒ–
- æ•…éšœæ’é™¤

**DevOps å·¥ç¨‹å¸ˆ**:
- CI/CD é›†æˆ
- è‡ªåŠ¨åŒ–è„šæœ¬
- ç›‘æ§å’Œå‘Šè­¦
- å®¹å™¨ç¼–æ’

**æ•°æ®ç§‘å­¦å®¶/ç ”ç©¶å‘˜**:
- ä½¿ç”¨ GPU èµ„æº
- æŠ¥å‘Šæ€§èƒ½é—®é¢˜
- æä¾›ä¼˜åŒ–åé¦ˆ

**ç³»ç»Ÿç®¡ç†å‘˜**:
- ç½‘ç»œé…ç½®
- å­˜å‚¨ç®¡ç†
- å®‰å…¨åŠ å›º
- å¤‡ä»½æ¢å¤

### æ²Ÿé€šæ¸ é“

**Slack é›†æˆ**:

```bash
# å‘é€å‘Šè­¦åˆ° Slack
send_slack_alert() {
    local message=$1
    local severity=$2

    curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
      -H 'Content-Type: application/json' \
      -d "{
        \"text\": \"GPU Alert\",
        \"attachments\": [{
          \"color\": \"${severity}\",
          \"text\": \"${message}\",
          \"ts\": $(date +%s)
        }]
      }"
}

# ä½¿ç”¨ç¤ºä¾‹
if [ "$GPU_TEMP" -gt 85 ]; then
    send_slack_alert "High GPU temperature: ${GPU_TEMP}Â°C on $(hostname)" "danger"
fi
```

### çŸ¥è¯†åº“ç»´æŠ¤

**Wiki ç»“æ„**ï¼ˆConfluence/GitBookï¼‰:

```
GPU Cluster Wiki
â”œâ”€â”€ Getting Started
â”‚   â”œâ”€â”€ Quick Start Guide
â”‚   â”œâ”€â”€ Access Request
â”‚   â””â”€â”€ First Job Tutorial
â”œâ”€â”€ User Guides
â”‚   â”œâ”€â”€ PyTorch on GPUs
â”‚   â”œâ”€â”€ Multi-GPU Training
â”‚   â””â”€â”€ NGC Containers
â”œâ”€â”€ Admin Guides
â”‚   â”œâ”€â”€ Deployment Guide
â”‚   â”œâ”€â”€ Upgrade Procedure
â”‚   â””â”€â”€ Troubleshooting
â”œâ”€â”€ Reference
â”‚   â”œâ”€â”€ Hardware Specs
â”‚   â”œâ”€â”€ Software Versions
â”‚   â””â”€â”€ Performance Baselines
â””â”€â”€ FAQ
    â”œâ”€â”€ Common Issues
    â””â”€â”€ Best Practices
```

---

## æ€»ç»“

### å…³é”®è¦ç‚¹

1. **é€‰æ‹©æ­£ç¡®çš„æ–¹æ³•**
   - å°è§„æ¨¡ â†’ Native
   - å¤§è§„æ¨¡ â†’ Precompiled
   - Kubernetes â†’ Driver Container

2. **è‡ªåŠ¨åŒ–ä¸€åˆ‡**
   - ä½¿ç”¨ Ansible è‡ªåŠ¨åŒ–éƒ¨ç½²
   - ä½¿ç”¨è„šæœ¬è‡ªåŠ¨åŒ–éªŒè¯
   - ä½¿ç”¨ç›‘æ§è‡ªåŠ¨åŒ–å‘Šè­¦

3. **å»ºç«‹åŸºçº¿å¹¶æŒç»­å¯¹æ¯”**
   - éƒ¨ç½²åç«‹å³å»ºç«‹åŸºçº¿
   - å®šæœŸè¿è¡Œæ€§èƒ½æµ‹è¯•
   - åŠæ—¶å‘ç°æ€§èƒ½é€€åŒ–

4. **åˆ†é˜¶æ®µéƒ¨ç½²**
   - æµ‹è¯• â†’ é‡‘ä¸é›€ â†’ ç”Ÿäº§
   - å°æ‰¹é‡ â†’ å¤§æ‰¹é‡
   - å§‹ç»ˆå‡†å¤‡å›æ»šæ–¹æ¡ˆ

5. **å®Œæ•´çš„æ–‡æ¡£**
   - è®°å½•æ‰€æœ‰é…ç½®
   - ä¿å­˜æ‰€æœ‰åŸºçº¿
   - è¿½è¸ªæ‰€æœ‰å˜æ›´

6. **æŒç»­ä¼˜åŒ–**
   - CPU ä¼˜åŒ–å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡
   - å®šæœŸå®¡æŸ¥é…ç½®
   - å…³æ³¨æ–°ç‰ˆæœ¬å’Œæ–°ç‰¹æ€§

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³æ‰§è¡Œ**:
- [ ] è¿è¡Œç¯å¢ƒæ£€æŸ¥
- [ ] é…ç½® Ansible inventory
- [ ] æ‰§è¡Œé¦–æ¬¡éƒ¨ç½²

**ç¬¬ä¸€å‘¨**:
- [ ] å»ºç«‹æ€§èƒ½åŸºçº¿
- [ ] é…ç½®ç›‘æ§
- [ ] è¿è¡Œå®Œæ•´éªŒè¯

**ç¬¬ä¸€ä¸ªæœˆ**:
- [ ] ä¼˜åŒ–é…ç½®
- [ ] å»ºç«‹è‡ªåŠ¨åŒ–æµç¨‹
- [ ] å®Œå–„æ–‡æ¡£

**æŒç»­è¿›è¡Œ**:
- [ ] ç›‘æ§å’Œå‘Šè­¦
- [ ] å®šæœŸéªŒè¯
- [ ] ç‰ˆæœ¬ç®¡ç†
- [ ] å›¢é˜ŸåŸ¹è®­

---

**æœ¬æœ€ä½³å®è·µæŒ‡å—å°†æŒç»­æ›´æ–°ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³» GPU é›†ç¾¤ç®¡ç†å›¢é˜Ÿã€‚**
