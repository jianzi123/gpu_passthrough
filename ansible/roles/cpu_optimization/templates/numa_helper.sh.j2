#!/bin/bash
# NUMA and GPU Affinity Helper Script
# Auto-generated by Ansible

echo "=========================================="
echo "NUMA and GPU Affinity Information"
echo "=========================================="

echo ""
echo "NUMA Topology:"
numactl --hardware

echo ""
echo "GPU NUMA Affinity:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi topo -m

    echo ""
    echo "GPU to NUMA Node Mapping:"
    for gpu in $(seq 0 $(($(nvidia-smi -L | wc -l) - 1))); do
        pci_id=$(nvidia-smi --id=$gpu --query-gpu=pci.bus_id --format=csv,noheader)
        pci_addr=$(echo $pci_id | tr '[:upper:]' '[:lower:]' | sed 's/^0000://')
        numa_node=$(cat /sys/bus/pci/devices/0000:$pci_addr/numa_node 2>/dev/null || echo "-1")
        gpu_name=$(nvidia-smi --id=$gpu --query-gpu=name --format=csv,noheader)
        echo "  GPU $gpu ($gpu_name): NUMA Node $numa_node (PCI: $pci_id)"
    done
else
    echo "nvidia-smi not found"
fi

echo ""
echo "=========================================="
echo "Usage Example:"
echo "=========================================="
echo "# Run on NUMA node 0:"
echo "numactl --cpunodebind=0 --membind=0 python train.py"
echo ""
echo "# Run multi-GPU with NUMA binding:"
echo "numactl --cpunodebind=0 --membind=0 python -m torch.distributed.launch --local_rank=0 train.py"
