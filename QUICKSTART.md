# å¿«é€Ÿå…¥é—¨æŒ‡å—

5 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²ç¬¬ä¸€å° GPU æœåŠ¡å™¨ï¼Œ30 åˆ†é’Ÿå®Œæˆå°å‹é›†ç¾¤éƒ¨ç½²ã€‚

## ğŸš€ 5 åˆ†é’Ÿï¼šå•èŠ‚ç‚¹å¿«é€Ÿéƒ¨ç½²

### å‰ææ¡ä»¶

- Ubuntu 22.04 æˆ– CentOS 8+
- è‡³å°‘ä¸€å— NVIDIA GPU
- sudo æƒé™
- ç½‘ç»œè¿æ¥

### å¿«é€Ÿæ­¥éª¤

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repo-url>
cd gpu_passthrough

# 2. å®‰è£… Ansibleï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
sudo apt update && sudo apt install -y ansible  # Ubuntu
# sudo yum install -y ansible  # CentOS

# 3. ä¸€é”®å®‰è£…ï¼ˆè‡ªåŠ¨æ£€æµ‹ GPU å¹¶å®‰è£…æœ€ä½³é©±åŠ¨ï¼‰
cd ansible
ansible-playbook -i localhost, -c local playbooks/setup_gpu_baseline.yml

# 4. é‡å¯ï¼ˆå¦‚æœæç¤ºéœ€è¦ï¼‰
sudo reboot

# 5. éªŒè¯
nvidia-smi
```

**å®Œæˆï¼** ä½ çš„ GPU æœåŠ¡å™¨å·²å°±ç»ªã€‚

---

## âš¡ 10 åˆ†é’Ÿï¼šå•èŠ‚ç‚¹ + ä¼˜åŒ– + éªŒè¯

### å®Œæ•´é…ç½®

```bash
# 1. å…‹éš†å¹¶è¿›å…¥é¡¹ç›®
git clone <repo-url>
cd gpu_passthrough

# 2. å®Œæ•´ä¼˜åŒ–éƒ¨ç½²
cd ansible
ansible-playbook -i localhost, -c local playbooks/full_deployment_optimized.yml

# é‡å¯åç»§ç»­...
sudo reboot

# 3. è¿è¡Œå®Œæ•´éªŒè¯
./scripts/validation/system_check.sh

# 4. æ€§èƒ½æµ‹è¯•
sudo /usr/local/bin/gpu-benchmark bandwidth
sudo /usr/local/bin/gpu-benchmark nccl

# 5. æ‹‰å– NGC é•œåƒï¼ˆå¯é€‰ï¼‰
./scripts/utils/ngc_manager.sh pull pytorch
./scripts/utils/ngc_manager.sh test pytorch
```

### éªŒè¯æ¸…å•

- âœ… nvidia-smi è¿è¡Œæ­£å¸¸
- âœ… CPU Governor è®¾ç½®ä¸º performance
- âœ… NUMA é…ç½®æ­£ç¡®
- âœ… PCIe/NVLink å¸¦å®½ç¬¦åˆé¢„æœŸ
- âœ… NGC é•œåƒå¯ä»¥è®¿é—® GPU

---

## ğŸ“¦ 30 åˆ†é’Ÿï¼šå°å‹é›†ç¾¤éƒ¨ç½²ï¼ˆ3-10 å°ï¼‰

### æ­¥éª¤ 1: å‡†å¤‡ Inventoryï¼ˆ5 åˆ†é’Ÿï¼‰

```bash
# åˆ›å»ºä¸»æœºæ¸…å•
cd ansible/inventory
cat > hosts << EOF
[gpu_nodes]
gpu-01 ansible_host=192.168.1.101
gpu-02 ansible_host=192.168.1.102
gpu-03 ansible_host=192.168.1.103

[all:vars]
ansible_user=ubuntu
ansible_become=true
EOF
```

### æ­¥éª¤ 2: é…ç½® SSH å…å¯†ç™»å½•ï¼ˆ5 åˆ†é’Ÿï¼‰

```bash
# ç”Ÿæˆ SSH å¯†é’¥ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
ssh-keygen -t rsa -b 4096

# å¤åˆ¶å…¬é’¥åˆ°æ‰€æœ‰èŠ‚ç‚¹
for i in gpu-0{1..3}; do
  ssh-copy-id ubuntu@${i}
done

# æµ‹è¯•è¿æ¥
ansible -i inventory/hosts all -m ping
```

### æ­¥éª¤ 3: æ‰¹é‡éƒ¨ç½²ï¼ˆ15 åˆ†é’Ÿï¼‰

```bash
# æ–¹æ³• 1: æ ‡å‡†éƒ¨ç½²
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml

# æ–¹æ³• 2: å®Œæ•´ä¼˜åŒ–éƒ¨ç½²ï¼ˆæ¨èï¼‰
ansible-playbook -i inventory/hosts playbooks/full_deployment_optimized.yml

# å¦‚æœéœ€è¦é‡å¯
ansible -i inventory/hosts all -m reboot

# ç­‰å¾…é‡å¯å®Œæˆ
sleep 120
```

### æ­¥éª¤ 4: éªŒè¯é›†ç¾¤ï¼ˆ5 åˆ†é’Ÿï¼‰

```bash
# æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹çš„é©±åŠ¨
ansible -i inventory/hosts all -m shell -a "nvidia-smi --query-gpu=name,driver_version --format=csv"

# è¿è¡ŒéªŒè¯ playbook
ansible-playbook -i inventory/hosts playbooks/validate_gpu.yml

# æ£€æŸ¥æŠ¥å‘Š
ls -lh /var/log/gpu_baseline/
```

### å®Œæˆï¼

ä½ çš„ GPU é›†ç¾¤å·²ç»readyï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼

---

## ğŸ¯ åœºæ™¯åŒ–å¿«é€Ÿå…¥é—¨

### åœºæ™¯ 1: æˆ‘åªæƒ³å¿«é€Ÿæµ‹è¯•ä¸€ä¸‹

```bash
# æœ€å°åŒ–å®‰è£…
cd ansible
ansible-playbook -i localhost, -c local playbooks/setup_gpu_baseline.yml \
  -e "install_cuda=false" \
  -e "install_container_runtime=false" \
  -e "run_post_install_validation=false"

# é‡å¯å¹¶éªŒè¯
sudo reboot
nvidia-smi
```

### åœºæ™¯ 2: æˆ‘éœ€è¦ç”¨äºæ·±åº¦å­¦ä¹ è®­ç»ƒ

```bash
# å®Œæ•´éƒ¨ç½² + NGC é•œåƒ
ansible-playbook -i localhost, -c local playbooks/full_deployment_optimized.yml

# é‡å¯
sudo reboot

# æ‹‰å–è®­ç»ƒé•œåƒ
./scripts/utils/ngc_manager.sh pull pytorch
./scripts/utils/ngc_manager.sh pull nemo

# æµ‹è¯•è®­ç»ƒ
docker run --gpus all --rm nvcr.io/nvidia/pytorch:24.01-py3 \
  python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
```

### åœºæ™¯ 3: æˆ‘è¦éƒ¨ç½²å¤§è§„æ¨¡é›†ç¾¤ï¼ˆ50+ å°ï¼‰

**æ¨èä½¿ç”¨é¢„ç¼–è¯‘é©±åŠ¨ï¼**

```bash
# 1. è¯†åˆ«é›†ç¾¤å†…æ ¸ç‰ˆæœ¬
ansible -i inventory/hosts all -m shell -a "uname -r" | \
  grep -v ">>" | sort -u > kernels.txt

# 2. é¢„æ„å»ºé©±åŠ¨ï¼ˆä¸€æ¬¡æ€§ï¼Œ2 å°æ—¶ï¼‰
./scripts/install/batch_build_drivers.sh

# 3. å»ºç«‹é©±åŠ¨ä»“åº“
mkdir -p /var/www/drivers
cp /opt/precompiled-drivers/* /var/www/drivers/
cd /var/www/drivers && python3 -m http.server 8080 &

# 4. åˆ†æ‰¹éƒ¨ç½²ï¼ˆæ¯æ‰¹ 10 å°ï¼Œæ¯æ‰¹ 20 åˆ†é’Ÿï¼‰
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml \
  -e "driver_installation_method=precompiled" \
  -e "precompiled_repo=http://repo-server:8080" \
  --limit batch1 \
  --forks 10

# é‡å¤ batch2, batch3...
```

**æ—¶é—´èŠ‚çœ**: ä¼ ç»Ÿæ–¹å¼ 50 å°æ—¶ â†’ é¢„ç¼–è¯‘æ–¹å¼ 3.5 å°æ—¶ï¼ˆ93% èŠ‚çœï¼ï¼‰

### åœºæ™¯ 4: Kubernetes GPU èŠ‚ç‚¹

```bash
# 1. éƒ¨ç½²é©±åŠ¨å®¹å™¨
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml \
  -e "driver_installation_method=driver-container"

# 2. å®‰è£… GPU Operator
helm repo add nvidia https://nvidia.github.io/gpu-operator
helm install gpu-operator nvidia/gpu-operator \
  -n gpu-operator-resources \
  --create-namespace

# 3. éªŒè¯
kubectl get pods -n gpu-operator-resources
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  containers:
  - name: cuda
    image: nvcr.io/nvidia/cuda:12.2.0-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
  restartPolicy: Never
EOF

kubectl logs gpu-test
```

---

## ğŸ”§ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

### é©±åŠ¨ç®¡ç†

```bash
# æŸ¥çœ‹é©±åŠ¨ç‰ˆæœ¬
nvidia-smi --query-gpu=driver_version --format=csv,noheader

# é¢„ç¼–è¯‘é©±åŠ¨ç®¡ç†
./scripts/utils/manage_precompiled_drivers.sh list          # åˆ—å‡ºå¯ç”¨é©±åŠ¨
./scripts/utils/manage_precompiled_drivers.sh install latest # å®‰è£…æœ€æ–°
./scripts/utils/manage_precompiled_drivers.sh rollback      # å›æ»š

# é©±åŠ¨å®¹å™¨ç®¡ç†
systemctl status nvidia-driver    # æŸ¥çœ‹çŠ¶æ€
systemctl restart nvidia-driver   # é‡å¯
journalctl -u nvidia-driver -f    # æŸ¥çœ‹æ—¥å¿—
```

### éªŒè¯å’Œæµ‹è¯•

```bash
# å¿«é€ŸéªŒè¯
./scripts/validation/quick_check.sh

# å®Œæ•´ç³»ç»Ÿæ£€æŸ¥
./scripts/validation/system_check.sh

# å¸¦å®½æµ‹è¯•
./scripts/validation/bandwidth_test.sh

# æ€§èƒ½åŸºå‡†
gpu-benchmark bandwidth   # PCIe/NVLink å¸¦å®½
gpu-benchmark nccl        # NCCL é€šä¿¡
gpu-benchmark megatron    # è®­ç»ƒåŸºå‡†
```

### NGC é•œåƒç®¡ç†

```bash
# åˆ—å‡ºå¯ç”¨é•œåƒ
./scripts/utils/ngc_manager.sh list

# æ‹‰å–é•œåƒ
./scripts/utils/ngc_manager.sh pull pytorch
./scripts/utils/ngc_manager.sh pull nemo
./scripts/utils/ngc_manager.sh pull triton

# è¿è¡Œé•œåƒ
./scripts/utils/ngc_manager.sh run pytorch

# æµ‹è¯• GPU è®¿é—®
./scripts/utils/ngc_manager.sh test pytorch
```

### Ansible å¿«æ·å‘½ä»¤

```bash
# æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹ GPU
ansible -i inventory/hosts all -m shell -a "nvidia-smi -L"

# æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
ansible -i inventory/hosts all -m shell -a "nvidia-smi --query-gpu=driver_version --format=csv"

# æ£€æŸ¥æ¸©åº¦
ansible -i inventory/hosts all -m shell -a "nvidia-smi --query-gpu=temperature.gpu --format=csv"

# è¿è¡Œè„šæœ¬
ansible -i inventory/hosts all -m script -a "./scripts/validation/quick_check.sh"

# é‡å¯æ‰€æœ‰èŠ‚ç‚¹
ansible -i inventory/hosts all -m reboot
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q: nvidia-smi æ‰¾ä¸åˆ°

```bash
# æ£€æŸ¥æ¨¡å—æ˜¯å¦åŠ è½½
lsmod | grep nvidia

# å¦‚æœæ²¡æœ‰ï¼Œæ‰‹åŠ¨åŠ è½½
sudo modprobe nvidia

# å¦‚æœä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥å®‰è£…
dpkg -l | grep nvidia-driver
```

### Q: é©±åŠ¨å®‰è£…åéœ€è¦é‡å¯å—ï¼Ÿ

**æ˜¯çš„**ï¼Œé¦–æ¬¡å®‰è£…é©±åŠ¨éœ€è¦é‡å¯ã€‚å¯ä»¥åœ¨ playbook ä¸­è‡ªåŠ¨å¤„ç†ï¼š

```bash
ansible-playbook -i inventory/hosts playbooks/setup_gpu_baseline.yml \
  -e "nvidia_driver_skip_reboot=false"
```

### Q: å¦‚ä½•é€‰æ‹©é©±åŠ¨å®‰è£…æ–¹æ³•ï¼Ÿ

ç®€å•è§„åˆ™ï¼š
- **< 10 å°**: Nativeï¼ˆç®€å•ï¼‰
- **10-50 å°**: Precompiledï¼ˆæ¨èï¼‰
- **> 50 å°**: Precompiledï¼ˆå¿…é¡»ï¼‰
- **Kubernetes**: Driver Container

### Q: æ€§èƒ½æµ‹è¯•ç»“æœä¸åŸºçº¿å·®å¼‚å¤§æ€ä¹ˆåŠï¼Ÿ

```bash
# 1. æ£€æŸ¥ CPU ä¼˜åŒ–æ˜¯å¦åº”ç”¨
./scripts/validation/system_check.sh | grep "CPU Governor"

# 2. æ£€æŸ¥ NUMA é…ç½®
nvidia-smi topo -m

# 3. é‡æ–°åº”ç”¨ä¼˜åŒ–
ansible-playbook -i inventory/hosts playbooks/apply_cpu_optimization.yml

# 4. é‡æ–°æµ‹è¯•
gpu-benchmark bandwidth
```

### Q: æ€ä¹ˆæ›´æ–°é©±åŠ¨ï¼Ÿ

```bash
# é¢„ç¼–è¯‘é©±åŠ¨
./scripts/utils/manage_precompiled_drivers.sh install <new-version>

# å¦‚æœå‡ºé—®é¢˜ï¼Œå›æ»š
./scripts/utils/manage_precompiled_drivers.sh rollback

# é©±åŠ¨å®¹å™¨
sudo systemctl stop nvidia-driver
# ä¿®æ”¹ /etc/systemd/system/nvidia-driver.service ä¸­çš„ç‰ˆæœ¬
sudo systemctl daemon-reload
sudo systemctl start nvidia-driver
```

---

## ğŸ“š ä¸‹ä¸€æ­¥

å®Œæˆå¿«é€Ÿå…¥é—¨åï¼Œå»ºè®®é˜…è¯»ï¼š

1. **è¯¦ç»†æ–‡æ¡£**
   - [æœ€ä½³å®è·µæŒ‡å—](docs/best_practices.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—
   - [é©±åŠ¨å®‰è£…æ–¹æ³•](docs/gpu_driver_installation_methods.md) - ä¸‰ç§æ–¹æ³•è¯¦è§£
   - [é¢„ç¼–è¯‘é©±åŠ¨æŒ‡å—](docs/precompiled_driver_guide.md) - å¤§è§„æ¨¡éƒ¨ç½²å¿…è¯»

2. **æ€§èƒ½ä¼˜åŒ–**
   - [CPU ä¼˜åŒ–](docs/latest_research_2025.md) - CPU æ€§èƒ½è°ƒä¼˜
   - [å¸¦å®½æµ‹è¯•](docs/bandwidth_and_benchmarks.md) - å¸¦å®½å’ŒåŸºå‡†æµ‹è¯•

3. **NGC å®¹å™¨**
   - [CUDA å…¼å®¹æ€§å’Œ NGC](docs/cuda_compatibility_and_ngc.md) - NGC é•œåƒä½¿ç”¨

## ğŸ†˜ è·å–å¸®åŠ©

- æŸ¥çœ‹æ–‡æ¡£: `docs/`
- è¿è¡Œç¤ºä¾‹: `examples/`
- é—®é¢˜åé¦ˆ: GitHub Issues
- å›¢é˜Ÿæ”¯æŒ: gpu-team@company.com

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼GPU é›†ç¾¤å·²ç»å‡†å¤‡å°±ç»ª ğŸš€**
